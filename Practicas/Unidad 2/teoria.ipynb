{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unidad 2\n",
    "### El problema de la comunicacion con la realidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cual es la base empirica? cual es el dato de la realidad?\n",
    "En la filosofia no hay.\n",
    "En la epistemologia hay cosas reales, tangibles.\n",
    "En las ciencias con datos (metodologicas), muchas veces nos basamos en teorias (ej: inflacion)\n",
    "\n",
    "La base empirica es dinamica y depende del conjunto de supuestos que una sociedad considera indudables (consenso, \"verdad\").\n",
    "\n",
    "Entonces... el dato se construye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dato como funcion proposicional: hay una unidad de analisis, a la que se le aplica una variabilidad (funcion)  y da un resultado. Ej: Altura(Gustavo)=1.71\n",
    "\n",
    "Yo quiero medir la funcion, por eso el resultado.\n",
    "\n",
    "La funcion es algo que muchas veces esta implicito (no claro). A veces los desacuerdos lo son x no aclarar a qué cosas nos referimos.\n",
    "\n",
    "La definicion de la funcion depende de la operatibilidad.\n",
    "\n",
    "Como llegamos de variable a resultado?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estructura invariante del dato empirico:\n",
    "\n",
    "Fuente: estado real de la variable\n",
    "Mensaje enviado: la dinamica del mundo.\n",
    "Transmisor (codificador): dimensiones perceptibles de la variable. Potencialmente medible. Son variables tambien.\n",
    "Perceptor: cuerpo o instrumento de medición. El procedimiento que capta la señal del transmisor\n",
    "Señal recibida: el indicador, lo efectivamente observado\n",
    "Receptor (decodificador): la hipotesis indicadora, como interpreto la señal.\n",
    "Mensaje recibido: la inferencia, la regla de la probailidad\n",
    "Destino: la estimacion (posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sistema de comuinicacion con la realidad (es lo de antes)\n",
    "de lo abstracto a lo concreto y de ahi a lo abstracto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teoria de la Informacion: el problema de comunicarse con la realidad\n",
    "\n",
    "No es directa, esta mediada por lo que decidimos: hipotesis, forma de tomar los datos, como los medimos, etc. Queremos \"eliminar el ruido entre el codificador y el decodificador\"\n",
    "\n",
    "\n",
    "\n",
    "Puedo acercarme para escuchar mejor (fisica) o interpretar la señal con ruido (inferencia, a esto le vamos a meter).\n",
    "\n",
    "\n",
    "\n",
    "Cómo hacemos esto? (inferencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ej: estimacion de habilidad en el siglo 20. (ELO)\n",
    "\n",
    "Habilidad (oculto)\n",
    "|\n",
    "v\n",
    "Desempeño (medible, depende de habilidad)\n",
    "|\n",
    "v\n",
    "Diferencia de desempeño (calculable)\n",
    "|\n",
    "v\n",
    "Resultado (ELO, resultado) Quien gana.\n",
    "\n",
    "Primero arrancan todos igual, despues usa la \"sorpresa\" para actualizar los valores de habilidad\n",
    "\n",
    "La sorpresa es un continuo, que tan esperable es que uno le gane al otro. En funcion de eso, aumenta mas o menos la nueva habilidad (o disminuye si perdes).\n",
    "\n",
    "Además tiene un parametro K, que es el learning rate. SI la conozco mucho a la persona, no actualizo tanto, si es desconocido, actualizo mas.\n",
    "\n",
    "Si aplico estrictamente las reglas de la probabilidad, la nueva informacion para modificar mi sistema de creencias (la likelihood), es la sorpresa.\n",
    "\n",
    "En el siglo 21 se le aplica una distribucion de probabilidad (incertidumbre) a la habilidad.\n",
    "\n",
    "VER DIAPOS\n",
    "\n",
    "Lo importante es el ejemplo y la sorpresa (como modificador de las ideas iniciales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informacion de Shannon:\n",
    "-log2P(Dato|Hipotesis)\n",
    "Cuando P va a 1, es poco informativo, I_sh va a 0\n",
    "Cuando P va a 0, es infinitamente informatico, Ish va a infinito (queire decir que nuestro modelo era totalmente falso!!!)\n",
    "\n",
    "Ejemplo: cantidad minima de preguntas para identificar numero entre 0 y 15...\n",
    "Rta: 4. Voy cortando a la mitad todo el espacio.\n",
    "\n",
    "x mod 16 >=8\n",
    "x mod 8 >=4\n",
    "x mod 4 >=2\n",
    "x mod 2 >=1\n",
    "\n",
    "log2 x bits para representar el numero x!!\n",
    "\n",
    "La informacion la mide en bits [-log2P(Dato|Hipotesis)]!!!\n",
    "\n",
    "Es basicamente, mide la informacion de un sistema y una serie de decisiones. \n",
    "La informacion en ordenes de magnitud!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosotros queremos comparar sistemas de comunicacion con la realidad (modelos).\n",
    "COmo? ya lo hicimos\n",
    "\n",
    "\n",
    "Ejemplo con Monty Hall y el modelo comun de la guia 1.1.\n",
    "\n",
    "Tasa de informacion de los sistemas de comunicacion\n",
    "\n",
    "P(Modelo_i, Datos_T) = P(M_i)P(Datos_T | M_i) La informacion esta en la sorpresa de la prediccion.\n",
    "Eso lo descomponiamos como el producto de las predicciones de los Datos individuales (Episodios)\n",
    "\n",
    "Podemos agrupar las observaciones con los mismos valores y usar un exponente de la cantidad de veces que aparecieron. Es como hacer la media geometrica para cada una de las combinaciones de los valores, le llamaremos tasaDePrediccion.\n",
    "\n",
    "Hacemos cuentas (ver PDF)\n",
    "\n",
    "Llegamos a la tasa de prediccion temporal en ordenes de magnitud. Lo importante es que hacemos T -> infinito, haciendo que la Probabilidad que usamos, no sea del modelo sino la de la realidad Causal, la realidad!!!\n",
    "\n",
    "Llegamos al Valor esperado de la informacion en ordenes de magnitud (Cross Entropy)\n",
    "Es probabilidad de que se genere el dato * Informacion del dato en ordenes de magnitud (Shannon)\n",
    "   (probabilidad de la realidad)               (probailidad del modelo causal)\n",
    "\n",
    "Idealmente queremos sistemas de comunicacion que no deban recibir informacion en absoluto (sabriamos todo). Los sistemas que mejor funcionan son los que reciben la menor cantidad de informacion.\n",
    "\n",
    "Entropia cruzada, cuando la realidad causal da la misma P que el modelo, la entropia cruzada se llama entropia. Sistema de comunicacion perfecto. No hay mejor tasa de prediccion que esta.\n",
    "Cuando son distintas, es Entropia Cruzada = Entropia + Divergencia (cuentas)\n",
    "\n",
    "ES LO DE ALGORITMOS 2 y Random Forest!!!!\n",
    "\n",
    "No se usa mucho la divergencia (porque depende de la realidad). Lo que si usamos es la entropia cruzada.\n",
    "Cross entropi penaliza correctamente los modelos que creen no ver cosas que si se ven\n",
    "\n",
    "Maxima entropia es con el argmax o argmin!! \n",
    "Es maximizar incertidumbre, minimizar la tasa de prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo: 12 pelotas y 1 balanza. Decidir cual es la pelota que pesa distinto que las otras.\n",
    "Elegir accion que use la balanza la menor cantidad de veces y ver si pesa mas o menos que el resto.\n",
    "\n",
    "Lo que hacemos es maximizar la entropia.\n",
    "\n",
    "La estimacion que hacemos nosotros (prediccion) es una aproximacion de la entropia cruzada!! (?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "1.561278124459133\n",
      "0.8112781244634514\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def entropia(ps):\n",
    "    return np.sum(ps* -np.log2(ps))\n",
    "\n",
    "print(entropia([1/4,1/2,1/4]))\n",
    "print(entropia([3/8,1/4,3/8])) #Este vale con ++- ++- (--)\n",
    "print(entropia([0.0000000000001,1/4,3/4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maxima entropia es la def formal del principio de no mentir.\n",
    "\n",
    "La familia de distribuciones exponenciales surgen de maximizar la entropia dadas ciertas condiciones.\n",
    "Ej: la gaussiana es maximizar la entropia dadas la media y la varianza.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divergencia Kullback-Leibler\n",
    "A partir de media geometrica en escala logaritmica, cuentas y llega a \n",
    "\n",
    "Entropia + Divergencia KL\n",
    "\n",
    "La Divergencia KL se puede descomponer como diferencia de logaritmos de las predicciones, pesado por la prediccion de alguno de los dos.\n",
    "\n",
    "De que sirve si no conozco la realidad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrueSkill: Posterior exacto, minima divergencia\n",
    "\n",
    "Como reducir la divergencia KL?\n",
    "Aproximo el posterior exacto con una gaussiana, minimizando la divergencia KL entre posterior exacto y el aproximado.\n",
    "\n",
    "Eso es TrueSkill. La proxima vamos a ver.\n",
    "\n",
    "De que me sirve??\n",
    "Si tengo un modelo, con una posterior que es horrible, es costoso computacionalmente hacer montecarlo. Entonces usando Trueskill, que es buscar la normal que aproxime mejor al posterior, minimizando la divergencia KL, lo que es computacionalmente mucho mas barato que las montecarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra cosa que podemos hacer es, en vez de modificar el modelo que interpreta la realidad, podemos \"acercarnos a escuchar mejor\" y comparar procedimientos de acercamiento a la realidad (Perceptor).\n",
    "Ejemplo, distintos tests de Chagas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la distribucion conjunta puedo sacar cualquier distribucion integrando, dependiendo de que quiero, integro unas u otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 1.9\n",
    "Definir las distribuciones, en funcion de eso las condicionales, producto de ellas las conjuntas y con eso saco todo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
